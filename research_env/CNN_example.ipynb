{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11da8e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce156353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import shutil\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "758b24c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory_path = \"Data\"\n",
    "data_zipfile_name = \"archive.zip\"\n",
    "data_zipfile_path = os.path.join(data_directory_path, data_zipfile_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c7c79473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with ZipFile(data_zipfile_path, \"r\") as zip_f:\n",
    "#     zip_f.extractall(\"Data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a2fa66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data\\training_set\\training_set\n"
     ]
    }
   ],
   "source": [
    "target_data_dirs = [\"cats\", \"dogs\"]\n",
    "parent_directory = os.path.join(data_directory_path, \"training_set\")\n",
    "parent_directory = os.path.join(parent_directory, \"training_set\")\n",
    "print(parent_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd9fbee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "BAD_DATA_DIR = \"Bad_data\"\n",
    "os.makedirs(BAD_DATA_DIR, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00f85913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding full path to the images and verifying the Images  \n",
    "\n",
    "for dirs in os.listdir(parent_directory):\n",
    "    full_path_data_dirs = os.path.join(parent_directory, dirs)\n",
    "    for img in os.listdir(full_path_data_dirs):\n",
    "        full_path_image = os.path.join(full_path_data_dirs, img)\n",
    "        try:\n",
    "            image = Image.open(full_path_image)\n",
    "            image.verify()\n",
    "#             print(f\"{full_path_image} --> is varified\")\n",
    "            \n",
    "        except Exception as e:\n",
    "#             print(f\"{full_path_image} --> is BAD\")\n",
    "            bad_data_path = os.path.join(BAD_DATA_DIR, img)\n",
    "            shutil.move(full_path_image, bad_data_path)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0431fb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d989d5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8005 files belonging to 2 classes.\n",
      "Using 6404 files for training.\n",
      "Found 8005 files belonging to 2 classes.\n",
      "Using 1601 files for validation.\n"
     ]
    }
   ],
   "source": [
    "#  img_dataset_from_directory will take the parent directory path\n",
    "#  it will split the data set, resize the images \n",
    "\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    parent_directory,\n",
    "    validation_split = 0.2,\n",
    "    subset = \"training\",\n",
    "    seed = 13,\n",
    "    image_size = IMG_SIZE,\n",
    "    batch_size = BATCH_SIZE,\n",
    ")\n",
    "\n",
    "valid_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    parent_directory,\n",
    "    validation_split = 0.2,\n",
    "    subset = \"validation\",\n",
    "    seed = 13,\n",
    "    image_size = IMG_SIZE,\n",
    "    batch_size = BATCH_SIZE,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c0ead8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  creating a unique name for log directory everytime\n",
    "\n",
    "def get_log_path(base_dir = os.path.join(\"log_CNN_example\", \"fit\")):\n",
    "    uniquename = time.asctime().replace(\" \", \"_\").replace(\":\", \"\")\n",
    "    log_path = os.path.join(base_dir, uniquename)\n",
    "    print(f\"saving logs at: {log_path}\")\n",
    "    return log_path    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e42ff758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving logs at: log_CNN_example\\fit\\Thu_May__9_184649_2024\n"
     ]
    }
   ],
   "source": [
    "log_dir = get_log_path()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ee0c147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 224, 224, 3) tf.Tensor([0 1 1 0 1 1 0 0 0 0 1 1 0 1 0 1 0 1 1 0 0 1 1 1 1 1 0 0 1 0 1 1], shape=(32,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "#  train_ds will give images and label\n",
    "#  labels it will select from folder names \n",
    "#  .take(1) will take only 1 batch (32 pics)\n",
    "\n",
    "for imgs, labels in train_ds.take(1):\n",
    "    print(imgs.shape, labels )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb90f256",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([32, 224, 224, 3])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bde0a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets create a file writer to see the images on tensor board\n",
    "\n",
    "file_writer = tf.summary.create_file_writer(logdir = log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e6e278ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "with file_writer.as_default():\n",
    "    images = np.array(imgs)\n",
    "    \n",
    "    tf.summary.image(\"samples\", images.astype(\"uint8\"), max_outputs = 10, step = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3adc5066",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  lets do the data augmentation\n",
    "#  we can use tf.keras.preprocessing.image.ImageDataGenerator()\n",
    "#  the other way is by adding a layer of augmetation\n",
    "\n",
    "AUG_STEPS = [\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomRotation(0.1)\n",
    "]\n",
    "\n",
    "data_aug_layer = tf.keras.Sequential(AUG_STEPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9afd8e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
